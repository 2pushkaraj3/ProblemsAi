{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOxSOnpgHzpEzYbAOTfV85W"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oe-ZOLOQynSB",
        "outputId": "829d4293-367a-48c6-ba5f-f714329bca31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Iterations:  6625\n",
            "VALUE FUNCTION:\n",
            "{0: 9473.398305163142, 1: 10377.756745551109, 2: 12627.657117756702, 3: 18149.10964969848, 4: 26888.101056763524, 5: 51042.565068385054, 6: 76514.78118309026, 7: 180786.20122217457, 8: 126830.12594201375, 9: 125850.34013605444}\n",
            "TIMES ENTERED:\n",
            "{0: 100.0, 1: 99.15471698113207, 2: 89.73584905660378, 3: 71.42641509433962, 4: 50.17358490566038, 5: 30.62641509433962, 6: 15.09433962264151, 7: 6.113207547169811, 8: 1.8415094339622642, 9: 0.3471698113207547}\n",
            "EXPECTED REWARD:  98555.19917935529\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class NODE:\n",
        "    def __init__(self, reward, correctProbability):\n",
        "        self.REWARD = reward\n",
        "        self.PROBABILITY = correctProbability\n",
        "        self.next = None\n",
        "\n",
        "class AGENT:\n",
        "    def __init__(self):\n",
        "        self.N = 10\n",
        "        self.THETA = 0.0001\n",
        "        self.DISCOUNT_FACTOR = 0.9\n",
        "        self.ACTIONS = [\"CONTINUE\", \"QUIT\"]\n",
        "        \n",
        "        REWARDS = [100, 500, 1000, 5000, 10000, 50000, 100000, 500000, 1000000, 5000000]\n",
        "        CORRECT_ANSWER_PROBABILITY = [0.99, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1]\n",
        "        \n",
        "        START_NODE = NODE(-1,0)\n",
        "        TEMP = START_NODE\n",
        "        \n",
        "        for i in range(self.N):  \n",
        "            TEMP.next = NODE(REWARDS[i],CORRECT_ANSWER_PROBABILITY[i])\n",
        "            TEMP = TEMP.next\n",
        "        \n",
        "        self.START_STATE = START_NODE.next        \n",
        "\n",
        "\n",
        "class MDP_SOLUTION:\n",
        "    def __init__(self):\n",
        "        self.agent = AGENT()\n",
        "        self.VALUE_FUNCTION = {s: 0 for s in range(self.agent.N)}\n",
        "        self.ITERATIONS = 0\n",
        "        self.TIMES_ENTERED = {s: 0 for s in range(self.agent.N)}\n",
        "        self.TERMINATOR = False\n",
        "        self.PLOT_STATES = [x for x in range(self.agent.N)]\n",
        "    \n",
        "    def helper(self, state, iteration):\n",
        "        if(state == None): return 0\n",
        "        self.TIMES_ENTERED[iteration] += 1\n",
        "    \n",
        "        OLD_VALUE = self.VALUE_FUNCTION[iteration]\n",
        "        REWARD_ACHIEVED = 0\n",
        "        if iteration == 0:\n",
        "            QUIT_REWARD = 0\n",
        "        else:\n",
        "            QUIT_REWARD = self.VALUE_FUNCTION[iteration-1]\n",
        "        \n",
        "        ANSWER = np.random.rand()\n",
        "        \n",
        "        if ANSWER <= state.PROBABILITY:\n",
        "            REWARD_ACHIEVED = state.PROBABILITY * (state.REWARD + (self.agent.DISCOUNT_FACTOR * self.helper(state.next, iteration+1)))\n",
        "            self.VALUE_FUNCTION[iteration] = (self.VALUE_FUNCTION[iteration] * self.TIMES_ENTERED[iteration] + REWARD_ACHIEVED)/(self.TIMES_ENTERED[iteration]+1)\n",
        "            \n",
        "            if(abs(self.VALUE_FUNCTION[iteration] - OLD_VALUE) < self.agent.THETA):\n",
        "                self.TERMINATOR = True\n",
        "\n",
        "        return max(QUIT_REWARD, REWARD_ACHIEVED)\n",
        "    \n",
        "    def solver(self):\n",
        "        while self.TERMINATOR == False:\n",
        "            self.ITERATIONS += 1\n",
        "            HEAD = self.agent.START_STATE\n",
        "            self.helper(HEAD, 0)\n",
        "        \n",
        "        print(\"Total Iterations: \", self.ITERATIONS)\n",
        "        print(\"VALUE FUNCTION:\")\n",
        "        print(self.VALUE_FUNCTION)\n",
        "        \n",
        "        for i in range(self.agent.N):\n",
        "            self.TIMES_ENTERED[i] = (self.TIMES_ENTERED[i] / self.ITERATIONS) * 100\n",
        "        \n",
        "        print(\"TIMES ENTERED:\")\n",
        "        print(self.TIMES_ENTERED)\n",
        "        \n",
        "        EXPECTATION = 0\n",
        "        for i in range(self.agent.N):\n",
        "            EXPECTATION = EXPECTATION + ((self.TIMES_ENTERED[i]/100) * self.VALUE_FUNCTION[i])\n",
        "        \n",
        "        print(\"EXPECTED REWARD: \", EXPECTATION)\n",
        "        \n",
        "\n",
        "MDP_SOLUTION().solver()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Define the problem parameters\n",
        "probabilities = [0.99, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1]\n",
        "rewards = [100, 500, 1000, 5000, 10000, 50000, 100000, 500000, 1000000, 5000000]\n",
        "\n",
        "# Define the number of simulations to run\n",
        "num_simulations = 10000\n",
        "\n",
        "# Define a function to simulate the game and return the total reward\n",
        "def simulate_game(stop_question):\n",
        "    total_reward = 0\n",
        "    for i in range(stop_question):\n",
        "        if random.random() < probabilities[i]:\n",
        "            total_reward += rewards[i]\n",
        "        else:\n",
        "            return -total_reward\n",
        "    return total_reward\n",
        "\n",
        "# Calculate the average reward for each question number\n",
        "avg_rewards = []\n",
        "for i in range(len(probabilities)):\n",
        "    total_reward = 0\n",
        "    for j in range(num_simulations):\n",
        "        total_reward += simulate_game(i+1)\n",
        "    avg_reward = total_reward / num_simulations\n",
        "    avg_rewards.append(avg_reward)\n",
        "\n",
        "# Find the question number that corresponds to the highest average reward\n",
        "max_reward = max(avg_rewards)\n",
        "max_question = avg_rewards.index(max_reward) + 1\n",
        "\n",
        "# Print the result\n",
        "print(\"The player should stop at question\", max_question+1, \"to maximize total reward.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6IcGdtoExB7",
        "outputId": "badb2e85-98f9-4555-9d3a-38fa07045666"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The player should stop at question 7 to maximize total reward.\n"
          ]
        }
      ]
    }
  ]
}